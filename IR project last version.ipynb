{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db188645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2e6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing and dict the terms\n",
    "documents = [\"1.txt\",\"2.txt\",\"3.txt\",\"4.txt\",\"5.txt\",\"6.txt\",\"7.txt\",\"8.txt\",\"9.txt\",\"10.txt\"]\n",
    "doc = [\"1.txt\"] # var to append each document after iterate\n",
    "numofdoc = 1 # var to count num of documents for each term\n",
    "t=[] # var for saving terms in it\n",
    "Terms = dict()  # dictionary for saving dictionary of each term\n",
    "for D in documents:\n",
    "    f = open(D, 'rt', encoding='utf-8')\n",
    "    f = f.read()\n",
    "    tokenized_word=word_tokenize(f)\n",
    "    r=0 # var for loop in index of each term in document\n",
    "    for i in tokenized_word:\n",
    "        l=[r] \n",
    "        trm = str(i)\n",
    "        if trm in t:\n",
    "            if D in doc:\n",
    "                Terms[trm].update({D:Terms[trm][D].append(r)})\n",
    "            else:\n",
    "                Terms[trm].update({D:l})\n",
    "        else:\n",
    "            dic = {\"term\": trm,\n",
    "                   \"number of docs containing term\":1,\n",
    "                D: l\n",
    "            }\n",
    "            Terms[trm] = dic\n",
    "            t.append(trm)\n",
    "        r+=1\n",
    "        Terms[trm].update({\"number of docs containing term\":len(Terms[trm].keys())-2})\n",
    "    doc.append(D)\n",
    "    numofdoc+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7f4ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antony': {'term': 'antony', 'number of docs containing term': 3, '1.txt': [0], '2.txt': [0], '6.txt': [0]}, 'brutus': {'term': 'brutus', 'number of docs containing term': 3, '1.txt': [1], '2.txt': [1], '4.txt': [0]}, 'caeser': {'term': 'caeser', 'number of docs containing term': 5, '1.txt': [2], '2.txt': [2], '4.txt': [1], '5.txt': [0], '6.txt': [1]}, 'cleopatra': {'term': 'cleopatra', 'number of docs containing term': 1, '1.txt': [3]}, 'mercy': {'term': 'mercy', 'number of docs containing term': 5, '1.txt': [4], '3.txt': [0], '4.txt': [2], '5.txt': [1], '6.txt': [2]}, 'worser': {'term': 'worser', 'number of docs containing term': 4, '1.txt': [5], '3.txt': [1], '4.txt': [3], '5.txt': [2]}, 'calpurnia': {'term': 'calpurnia', 'number of docs containing term': 1, '2.txt': [3]}, 'angels': {'term': 'angels', 'number of docs containing term': 3, '7.txt': [0], '8.txt': [0], '9.txt': [0]}, 'fools': {'term': 'fools', 'number of docs containing term': 4, '7.txt': [1], '8.txt': [1], '9.txt': [1], '10.txt': [0]}, 'fear': {'term': 'fear', 'number of docs containing term': 3, '7.txt': [2], '8.txt': [2], '10.txt': [1]}, 'in': {'term': 'in', 'number of docs containing term': 4, '7.txt': [3], '8.txt': [3], '9.txt': [2], '10.txt': [2]}, 'rush': {'term': 'rush', 'number of docs containing term': 4, '7.txt': [4], '8.txt': [4], '9.txt': [3], '10.txt': [3]}, 'to': {'term': 'to', 'number of docs containing term': 4, '7.txt': [5], '8.txt': [5], '9.txt': [4], '10.txt': [4]}, 'tread': {'term': 'tread', 'number of docs containing term': 4, '7.txt': [6], '8.txt': [6], '9.txt': [5], '10.txt': [5]}, 'where': {'term': 'where', 'number of docs containing term': 4, '7.txt': [7], '8.txt': [7], '9.txt': [6], '10.txt': [6]}}\n"
     ]
    }
   ],
   "source": [
    "print(Terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be4f62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'term': 'antony', 'number of docs containing term': 3, '1.txt': [0], '2.txt': [0], '6.txt': [0]}\n"
     ]
    }
   ],
   "source": [
    "print(Terms[\"antony\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb903c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to define empty list for var\n",
    "def define_lists():\n",
    "    table = [[None for _ in range(numofdoc-1)] for _ in range(len(t))]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7fd43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for get term freq in table\n",
    "def get_TermFreq(term,document):\n",
    "    table = define_lists() \n",
    "    for i in term:\n",
    "        for x in document:\n",
    "            if(x in Terms[i]):\n",
    "                table[term.index(i)][document.index(x)] = len(Terms[i][x])\n",
    "            else:\n",
    "                table[term.index(i)][document.index(x)] = 0\n",
    "    return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e90cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 0, 0, 0, 1, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 1, 1, 1, 0, 0, 0, 0], [1, 0, 1, 1, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 0, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
      "['antony', 'brutus', 'caeser', 'cleopatra', 'mercy', 'worser', 'calpurnia', 'angels', 'fools', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n"
     ]
    }
   ],
   "source": [
    "termFrequency = get_TermFreq(t,documents)\n",
    "print(termFrequency)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d8dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for get weight term freq in table\n",
    "def get_Weight_TermFreq(term,document,TF):\n",
    "    table = define_lists()\n",
    "    for i in term:\n",
    "        for x in document:\n",
    "            if(x in Terms[i] and table[term.index(i)][document.index(x)] != 0):\n",
    "                table[term.index(i)][document.index(x)] = (1 + math.log(TF[term.index(i)][document.index(x)],10))\n",
    "            else:\n",
    "                table[term.index(i)][document.index(x)] = 0\n",
    "    return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd0df8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 1.0, 0, 0, 0, 1.0, 0, 0, 0, 0], [1.0, 1.0, 0, 1.0, 0, 0, 0, 0, 0, 0], [1.0, 1.0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 0], [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1.0, 0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 0], [1.0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 0, 0], [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 0], [0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0], [0, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 1.0], [0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0], [0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0], [0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0], [0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0], [0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0]]\n",
      "['antony', 'brutus', 'caeser', 'cleopatra', 'mercy', 'worser', 'calpurnia', 'angels', 'fools', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n"
     ]
    }
   ],
   "source": [
    "W_termFrequency=get_Weight_TermFreq(t,documents,termFrequency)\n",
    "print(W_termFrequency)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b849b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get idf in list\n",
    "def get_idf(term,document):\n",
    "    idf=list()\n",
    "    for i in term:\n",
    "        idf.append(math.log((len(document))/Terms[i][\"number of docs containing term\"],10))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c48a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5228787452803376, 0.5228787452803376, 0.30102999566398114, 1.0, 0.30102999566398114, 0.3979400086720376, 1.0, 0.5228787452803376, 0.3979400086720376, 0.5228787452803376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376]\n",
      "['antony', 'brutus', 'caeser', 'cleopatra', 'mercy', 'worser', 'calpurnia', 'angels', 'fools', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n"
     ]
    }
   ],
   "source": [
    "idf = get_idf(t,documents)\n",
    "print(idf)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43ffdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get tf*idf in table\n",
    "def get_tf_idf(term,document,W_TF,I_df):\n",
    "    table = define_lists()\n",
    "    for i in term:\n",
    "        for x in document:\n",
    "            if(x in Terms[i]):\n",
    "                table[term.index(i)][document.index(x)] = W_TF[term.index(i)][document.index(x)]*I_df[term.index(i)]\n",
    "            else:\n",
    "                table[term.index(i)][document.index(x)] = 0\n",
    "    return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5ce1915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5228787452803376, 0.5228787452803376, 0, 0, 0, 0.5228787452803376, 0, 0, 0, 0], [0.5228787452803376, 0.5228787452803376, 0, 0.5228787452803376, 0, 0, 0, 0, 0, 0], [0.30102999566398114, 0.30102999566398114, 0, 0.30102999566398114, 0.30102999566398114, 0.30102999566398114, 0, 0, 0, 0], [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0.30102999566398114, 0, 0.30102999566398114, 0.30102999566398114, 0.30102999566398114, 0.30102999566398114, 0, 0, 0, 0], [0.3979400086720376, 0, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0, 0, 0, 0, 0], [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.5228787452803376, 0.5228787452803376, 0.5228787452803376, 0], [0, 0, 0, 0, 0, 0, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376], [0, 0, 0, 0, 0, 0, 0.5228787452803376, 0.5228787452803376, 0, 0.5228787452803376], [0, 0, 0, 0, 0, 0, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376], [0, 0, 0, 0, 0, 0, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376], [0, 0, 0, 0, 0, 0, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376], [0, 0, 0, 0, 0, 0, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376], [0, 0, 0, 0, 0, 0, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376, 0.3979400086720376]]\n",
      "['antony', 'brutus', 'caeser', 'cleopatra', 'mercy', 'worser', 'calpurnia', 'angels', 'fools', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n"
     ]
    }
   ],
   "source": [
    "tf_idf = get_tf_idf(t,documents,W_termFrequency,idf)\n",
    "print(tf_idf)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1740565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate document length\n",
    "def document_len(table,document):\n",
    "    doc_list = list()\n",
    "    doc_len = list()\n",
    "    for i in range(len(document)):\n",
    "        summition = 0\n",
    "        for x in range(len(t)):\n",
    "            doc_list = table[x][i]\n",
    "            summition = summition + pow(doc_list,2)\n",
    "        doc_len.append(math.sqrt(summition))\n",
    "    return doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba2ed17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3734623153231016, 1.2796184676775093, 0.49897425664192124, 0.782940961597204, 0.5827472583211476, 0.674270197209437, 1.2234957570597818, 1.2234957570597818, 1.1061372813884127, 1.1061372813884127]\n",
      "['antony', 'brutus', 'caeser', 'cleopatra', 'mercy', 'worser', 'calpurnia', 'angels', 'fools', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n"
     ]
    }
   ],
   "source": [
    "d_len = document_len(tf_idf,documents)\n",
    "print(d_len)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2235c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate Normalized tf*idf in table\n",
    "def get_Normalized_tf_idf(term,document,TF_IDF,document_len):\n",
    "    table = define_lists()\n",
    "    for i in term:\n",
    "        for x in document:\n",
    "            if(x in Terms[i]):\n",
    "                table[term.index(i)][document.index(x)] = TF_IDF[term.index(i)][document.index(x)] / document_len[document.index(x)]\n",
    "            else:\n",
    "                table[term.index(i)][document.index(x)] = 0\n",
    "    return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c05d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38070119539998626, 0.40862081822666696, 0, 0, 0, 0.775473611979805, 0, 0, 0, 0], [0.38070119539998626, 0.40862081822666696, 0, 0.6678393019745218, 0, 0, 0, 0, 0, 0], [0.21917601400892098, 0.23524980552238092, 0, 0.3844862006579375, 0.5165704194494654, 0.4464530642312778, 0, 0, 0, 0], [0.7280869586616608, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0.21917601400892098, 0, 0.6032976484396252, 0.3844862006579375, 0.5165704194494654, 0.4464530642312778, 0, 0, 0, 0], [0.2897349306438188, 0, 0.7975161110518196, 0.5082631107462275, 0.6828689504579974, 0, 0, 0, 0, 0], [0, 0.7814829382815854, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.4273645758583444, 0.4273645758583444, 0.47270691809973653, 0], [0, 0, 0, 0, 0, 0, 0.32524837652754823, 0.32524837652754823, 0.3597564383442055, 0.3597564383442055], [0, 0, 0, 0, 0, 0, 0.4273645758583444, 0.4273645758583444, 0, 0.47270691809973653], [0, 0, 0, 0, 0, 0, 0.32524837652754823, 0.32524837652754823, 0.3597564383442055, 0.3597564383442055], [0, 0, 0, 0, 0, 0, 0.32524837652754823, 0.32524837652754823, 0.3597564383442055, 0.3597564383442055], [0, 0, 0, 0, 0, 0, 0.32524837652754823, 0.32524837652754823, 0.3597564383442055, 0.3597564383442055], [0, 0, 0, 0, 0, 0, 0.32524837652754823, 0.32524837652754823, 0.3597564383442055, 0.3597564383442055], [0, 0, 0, 0, 0, 0, 0.32524837652754823, 0.32524837652754823, 0.3597564383442055, 0.3597564383442055]]\n",
      "['antony', 'brutus', 'caeser', 'cleopatra', 'mercy', 'worser', 'calpurnia', 'angels', 'fools', 'fear', 'in', 'rush', 'to', 'tread', 'where']\n"
     ]
    }
   ],
   "source": [
    "Normalized_tf_idf = get_Normalized_tf_idf(t,documents,tf_idf,d_len)\n",
    "print(Normalized_tf_idf)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deffe122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_documentsMatchQuery(ex):\\n    returned_docs=list()\\n    term1 , term2 = ex.split()\\n    if(term1 in Terms and term2 in Terms):\\n        for i in documents:\\n            if(i in Terms[term1] and i in Terms[term2]):\\n                l1 = Terms[term1][i]\\n                l2 = Terms[term2][i]\\n                for x2 in l2:\\n                    for x1 in l1:\\n                        if(x1 == x2-1):\\n                            returned_docs.append(i)     \\n        returned_docs = list(set(returned_docs))\\n        all_data_for_doc=[]\\n        for z in returned_docs:\\n            data_for_term=[]\\n            for i in term1,term2:\\n                tf_raw=termFrequency[t.index(i)][documents.index(z)]\\n                w_tf=W_termFrequency[t.index(i)][documents.index(z)]\\n                IDF=idf[documents.index(z)]\\n                TF_IDF = tf_idf[t.index(i)][documents.index(z)]\\n                norm_TF_IDF=Normalized_tf_idf[t.index(i)][documents.index(z)]\\n                data_for_term=[i,tf_raw,w_tf,IDF,TF_IDF,norm_TF_IDF,z]\\n                all_data_for_doc.append(data_for_term)\\n    return all_data_for_doc\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for return all data of query for each term in each document\n",
    "\"\"\"\n",
    "def get_documentsMatchQuery(ex):\n",
    "    returned_docs=list()\n",
    "    term1 , term2 = ex.split()\n",
    "    if(term1 in Terms and term2 in Terms):\n",
    "        for i in documents:\n",
    "            if(i in Terms[term1] and i in Terms[term2]):\n",
    "                l1 = Terms[term1][i]\n",
    "                l2 = Terms[term2][i]\n",
    "                for x2 in l2:\n",
    "                    for x1 in l1:\n",
    "                        if(x1 == x2-1):\n",
    "                            returned_docs.append(i)     \n",
    "        returned_docs = list(set(returned_docs))\n",
    "        all_data_for_doc=[]\n",
    "        for z in returned_docs:\n",
    "            data_for_term=[]\n",
    "            for i in term1,term2:\n",
    "                tf_raw=termFrequency[t.index(i)][documents.index(z)]\n",
    "                w_tf=W_termFrequency[t.index(i)][documents.index(z)]\n",
    "                IDF=idf[documents.index(z)]\n",
    "                TF_IDF = tf_idf[t.index(i)][documents.index(z)]\n",
    "                norm_TF_IDF=Normalized_tf_idf[t.index(i)][documents.index(z)]\n",
    "                data_for_term=[i,tf_raw,w_tf,IDF,TF_IDF,norm_TF_IDF,z]\n",
    "                all_data_for_doc.append(data_for_term)\n",
    "    return all_data_for_doc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c85e91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for return all data of query for each term in each document\n",
    "def get_documentsMatchQuery(ex):\n",
    "    returned_docs=list()\n",
    "    term = ex.split()\n",
    "    for x in term:\n",
    "        if x not in Terms:\n",
    "            return 0    \n",
    "    for i in range(len(term)-1):\n",
    "        for d in documents:\n",
    "            if(d in Terms[term[i]] and d in Terms[term[i+1]]):\n",
    "                l1 = Terms[term[i]][d]\n",
    "                l2 = Terms[term[i+1]][d]\n",
    "                for x2 in l2:\n",
    "                    for x1 in l1:\n",
    "                        if(x1 == x2-1):\n",
    "                            returned_docs.append(d) \n",
    "    returned_docs = list(set(returned_docs))\n",
    "    return returned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05bcab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.txt', '1.txt']\n"
     ]
    }
   ],
   "source": [
    "data = get_documentsMatchQuery(\"antony brutus\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4397b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cos_sim(data,ex):\n",
    "    #first get data of query\n",
    "    vector_of_query_per_doc=list()\n",
    "    tf_idf_vector=list()\n",
    "    sum_of_tf_idf_vector = 0\n",
    "    term = ex.split()\n",
    "    vectors_of_doc = dict()\n",
    "    vectors_of_cosine=dict()\n",
    "    data_of_terms=dict()\n",
    "    data_for_term = dict()\n",
    "    for i in term:\n",
    "        tf_raw = term.count(i)\n",
    "        w_tf = (1 + math.log(tf_raw,10))\n",
    "        idf_for_term = idf[t.index(i)]\n",
    "        w_tf_idf = w_tf * idf_for_term\n",
    "        tf_idf_vector.append(w_tf_idf)\n",
    "        data_for_term={\n",
    "            \"tf-raw\" : tf_raw,\n",
    "            \"w tf(1+ log tf)\" : w_tf,\n",
    "            \"idf\" : idf_for_term,\n",
    "            \"tf*idf\" : w_tf_idf\n",
    "        }\n",
    "        data_of_terms[i] = data_for_term\n",
    "        \n",
    "    #query len\n",
    "    for x in tf_idf_vector:\n",
    "        sum_of_tf_idf_vector = sum_of_tf_idf_vector + pow(x,2)\n",
    "        \n",
    "    query_len= math.sqrt(sum_of_tf_idf_vector)\n",
    "    \n",
    "    #query vector\n",
    "    for x in tf_idf_vector:\n",
    "        vector_of_query_per_doc.append(x/query_len)\n",
    "    \n",
    "    # append column of normalized\n",
    "    i = 0\n",
    "    for normalized in vector_of_query_per_doc:\n",
    "        data_of_terms[term[i]][\"normalized\"] = normalized\n",
    "        i = i+1\n",
    "        \n",
    "    #print data of query\n",
    "    print(data_of_terms)\n",
    "    #data of each doc\n",
    "    for D in data:\n",
    "        for i in term:\n",
    "            r=Normalized_tf_idf[t.index(i)][documents.index(D)]\n",
    "            l=[r]\n",
    "            if D in vectors_of_doc:\n",
    "                vectors_of_doc[D].append(r)\n",
    "                vectors_of_doc.update({D:vectors_of_doc[D]})\n",
    "            else:\n",
    "                vectors_of_doc.update({D:l})\n",
    "    #get cosine sim    \n",
    "    for x, y in vectors_of_doc.items():\n",
    "        vectors_of_cosine.update({x:np.dot(vector_of_query_per_doc, y)})\n",
    "    return vectors_of_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a63ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antony': {'tf-raw': 1, 'w tf(1+ log tf)': 1.0, 'idf': 0.5228787452803376, 'tf*idf': 0.5228787452803376, 'normalized': 0.7071067811865475}, 'brutus': {'tf-raw': 1, 'w tf(1+ log tf)': 1.0, 'idf': 0.5228787452803376, 'tf*idf': 0.5228787452803376, 'normalized': 0.7071067811865475}}\n",
      "cosine similarity :  {'2.txt': 0.5778771030041435, '1.txt': 0.5383927937463102}\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = cos_sim(data,\"antony brutus\")\n",
    "print(\"cosine similarity : \",cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac041882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ranking(rank):\n",
    "    rank=sorted(rank)\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66311a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.txt', '2.txt']\n"
     ]
    }
   ],
   "source": [
    "print(Ranking(cosine_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15888e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
